{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbce637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\LLM\\LangGraph_Demo\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from operator import add as add_messages\n",
    "from google import genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68bc25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cf496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_pdf(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "    \"\"\"Load PDF and split it into chunks for processing.\"\"\"\n",
    "    pdf_loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    try:\n",
    "        pages = pdf_loader.load()\n",
    "        print(f\"PDF has been loaded and has {len(pages)} pages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    pages_split = text_splitter.split_documents(pages)\n",
    "    \n",
    "    return pages_split\n",
    "\n",
    "def setup_vector_store(documents, embeddings, persist_directory: str, collection_name: str):\n",
    "    \"\"\"Create ChromaDB vector store and return retriever.\"\"\"\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "\n",
    "    try:\n",
    "        # Here, we actually create the chroma database using our embeddings model\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        print(f\"Created ChromaDB vector store!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up ChromaDB: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Now we create our retriever with improved search parameters\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 10}\n",
    "    )\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74bff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has been loaded and has 2 pages\n",
      "Created ChromaDB vector store!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "pdf_path = \"NBA_2024_25_Season_Summary.pdf\"\n",
    "persist_directory = os.getcwd()\n",
    "collection_name = \"nba_summary\"\n",
    "\n",
    "# Load and process PDF\n",
    "pages_split = load_and_process_pdf(pdf_path)\n",
    "\n",
    "# Setup vector store and retriever\n",
    "retriever = setup_vector_store(pages_split, embeddings, persist_directory, collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8108b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def math_calculator(operation: str, a: float, b: float = None):\n",
    "    \"\"\"\n",
    "    Perform basic mathematical operations.\n",
    "    \n",
    "    Args:\n",
    "        operation: The math operation to perform. Options: 'add', 'subtract', 'multiply', 'divide', 'power', 'sqrt', 'abs', 'round'\n",
    "        a: First number (required)\n",
    "        b: Second number (required for binary operations like add, subtract, multiply, divide, power)\n",
    "    \n",
    "    Returns:\n",
    "        String representation of the calculation result\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    operation = operation.lower().strip()\n",
    "    \n",
    "    try:\n",
    "        if operation in ['add', '+', 'plus']:\n",
    "            if b is None:\n",
    "                return \"Error: Addition requires two numbers\"\n",
    "            result = a + b\n",
    "            return f\"{a} + {b} = {result}\"\n",
    "            \n",
    "        elif operation in ['subtract', '-', 'minus']:\n",
    "            if b is None:\n",
    "                return \"Error: Subtraction requires two numbers\"\n",
    "            result = a - b\n",
    "            return f\"{a} - {b} = {result}\"\n",
    "            \n",
    "        elif operation in ['multiply', '*', 'times']:\n",
    "            if b is None:\n",
    "                return \"Error: Multiplication requires two numbers\"\n",
    "            result = a * b\n",
    "            return f\"{a} Ã— {b} = {result}\"\n",
    "            \n",
    "        elif operation in ['divide', '/', 'divided by']:\n",
    "            if b is None:\n",
    "                return \"Error: Division requires two numbers\"\n",
    "            if b == 0:\n",
    "                return \"Error: Division by zero is not allowed\"\n",
    "            result = a / b\n",
    "            return f\"{a} Ã· {b} = {result}\"   \n",
    "        else:\n",
    "            return f\"Error: Unknown operation '{operation}'. Supported operations: add, subtract, multiply, divide, power, sqrt, abs, round, sin, cos, tan\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error performing calculation: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents from the vector store based on the query. Use this document to answer questions about the NBA 2024-25 season.\"\"\"\n",
    "    \n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    if not docs:\n",
    "        return f\"I found no relevant information for '{query}' in the document. Try rephrasing your question or ask about a different topic.\"\n",
    "    \n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):   \n",
    "        results.append(f\"Document {i+1}:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53353289",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_documents, math_calculator] \n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the last message contains tool calls.\"\"\"\n",
    "    result = state['messages'][-1]\n",
    "    return hasattr(result, 'tool_calls') and len(result.tool_calls) > 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e18bb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an AI assistant with access to a PDF document loaded into your knowledge base. \n",
    "Your primary responsibility is to answer user questions based on the content of that PDF. \n",
    "Always try to ground your responses in the document first. If the information can be found in the PDF, \n",
    "provide a clear, accurate, and well-structured answer that summarizes or references the relevant sections. \n",
    "If the information is not present in the document, notify the user that the PDF does not cover it, \n",
    "and then do your best to answer from your own knowledge. \n",
    "You may also use any additional tools that are available to improve the accuracy or completeness of your response. \n",
    "Your goal is to prioritize the PDFs content while ensuring the user always receives a helpful answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436420ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tool registry for better organization\n",
    "available_tools = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Conditional edge function\n",
    "def needs_tool_execution(state: AgentState) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if the last message contains tool calls that need to be executed.\n",
    "    This controls the flow between reasoning and tool execution in the RAG pipeline.\n",
    "    \"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    return hasattr(last_message, 'tool_calls') and len(last_message.tool_calls) > 0 \n",
    "\n",
    "# LLM Reasoning Agent\n",
    "def agent_reasoning(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Core reasoning function that processes user input and decides on actions.\n",
    "    This represents the 'Generate' step in RAG (Retrieval-Augmented Generation).\n",
    "    \"\"\"\n",
    "    messages = list(state['messages'])\n",
    "    messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    response = llm.invoke(messages)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "\n",
    "# Tool Execution Agent  \n",
    "def execute_tools(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Execute tool calls from the LLM's response.\n",
    "    This handles both retrieval tools (R in RAG) and other utility tools.\n",
    "    \"\"\"\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    print(f\"Executing {len(tool_calls)} tool call(s)...\")\n",
    "    \n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        print(f\"Calling Tool: {tool_call['name']} with args: {tool_call['args']}\")\n",
    "        \n",
    "        if tool_call['name'] not in available_tools:\n",
    "            print(f\"Error: Tool '{tool_call['name']}' does not exist.\")\n",
    "            result = f\"Error: Tool '{tool_call['name']}' is not available. Available tools: {list(available_tools.keys())}\"\n",
    "        else:\n",
    "            try:\n",
    "                result = available_tools[tool_call['name']].invoke(tool_call['args'])\n",
    "                print(f\"Tool execution successful. Result length: {len(str(result))} characters\")\n",
    "            except Exception as e:\n",
    "                result = f\"Error executing tool '{tool_call['name']}': {str(e)}\"\n",
    "                print(f\"Tool execution failed: {e}\")\n",
    "\n",
    "        # Create tool message with result\n",
    "        tool_message = ToolMessage(\n",
    "            tool_call_id=tool_call['id'], \n",
    "            name=tool_call['name'], \n",
    "            content=str(result)\n",
    "        )\n",
    "        results.append(tool_message)\n",
    "\n",
    "    print(\"All tools executed. Returning to reasoning agent...\")\n",
    "    return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07830bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG Agent Graph\n",
    "rag_graph = StateGraph(AgentState)\n",
    "rag_graph.add_node(\"reasoning\", agent_reasoning)\n",
    "rag_graph.add_node(\"tool_execution\", execute_tools)\n",
    "\n",
    "# Define the conditional flow: reasoning -> tool_execution (if needed) -> reasoning -> end\n",
    "rag_graph.add_conditional_edges(\n",
    "    \"reasoning\",\n",
    "    needs_tool_execution,\n",
    "    {True: \"tool_execution\", False: END}\n",
    ")\n",
    "rag_graph.add_edge(\"tool_execution\", \"reasoning\")\n",
    "rag_graph.set_entry_point(\"reasoning\")\n",
    "\n",
    "# Compile the RAG agent\n",
    "rag_agent = rag_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da0f7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationMemory:\n",
    "    \"\"\"Manages conversation history with configurable memory length.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_messages: int = 20):\n",
    "        \"\"\"\n",
    "        Initialize conversation memory.\n",
    "        \n",
    "        Args:\n",
    "            max_messages: Maximum number of messages to keep in memory.\n",
    "                         Default is 20 (about 10 exchanges), which provides good\n",
    "                         context while staying within token limits.\n",
    "        \"\"\"\n",
    "        self.max_messages = max_messages\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def add_message(self, message: BaseMessage):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.conversation_history.append(message)\n",
    "        self._trim_history()\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]):\n",
    "        \"\"\"Add multiple messages to the conversation history.\"\"\"\n",
    "        self.conversation_history.extend(messages)\n",
    "        self._trim_history()\n",
    "    \n",
    "    def get_history(self) -> list[BaseMessage]:\n",
    "        \"\"\"Get the current conversation history.\"\"\"\n",
    "        return self.conversation_history.copy()\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear all conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def _trim_history(self):\n",
    "        \"\"\"Keep only the most recent messages within the memory limit.\"\"\"\n",
    "        if len(self.conversation_history) > self.max_messages:\n",
    "            # Keep the most recent messages\n",
    "            self.conversation_history = self.conversation_history[-self.max_messages:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e54ea6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_agent_with_memory(memory_length: int = 20):\n",
    "    \"\"\"\n",
    "    Main function to run the RAG (Retrieval-Augmented Generation) agent with conversation memory.\n",
    "    \n",
    "    This function implements the complete RAG pipeline:\n",
    "    1. Retrieval: Search relevant documents from the knowledge base\n",
    "    2. Augmentation: Combine retrieved context with user query  \n",
    "    3. Generation: Generate response using LLM with retrieved context\n",
    "    \n",
    "    Args:\n",
    "        memory_length: Number of recent messages to remember (default: 20).\n",
    "                      This equals about 10 exchanges, providing good context\n",
    "                      while staying within reasonable token limits.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ¤– RAG AGENT - Retrieval Augmented Generation\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ðŸ“š Knowledge Base: NBA 2024-25 Season Summary\")\n",
    "    print(f\"ðŸ’¬ Type 'exit' or 'quit' to end the conversation\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize conversation memory management\n",
    "    conversation_memory = ConversationMemory(max_messages=memory_length)\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(f\"\\nðŸ’­ Your question: \")\n",
    "        \n",
    "        if user_query.lower().strip() in ['exit', 'quit', 'bye']:\n",
    "            break\n",
    "            \n",
    "        # Add user message to conversation memory\n",
    "        user_message = HumanMessage(content=user_query)\n",
    "        conversation_memory.add_message(user_message)\n",
    "        \n",
    "        # Get full conversation history and invoke RAG agent\n",
    "        conversation_history = conversation_memory.get_history()\n",
    "        \n",
    "        try:\n",
    "            # Run the RAG pipeline\n",
    "            result = rag_agent.invoke({\"messages\": conversation_history})\n",
    "            \n",
    "            # Extract and store the final response\n",
    "            final_response = result['messages'][-1]\n",
    "            conversation_memory.add_message(final_response)\n",
    "            \n",
    "            # Display the response\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ðŸ¤– AGENT RESPONSE:\")\n",
    "            print(\"=\"*50)\n",
    "            print(final_response.content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error processing your request: {str(e)}\")\n",
    "            print(\"Please try rephrasing your question or contact support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "619b0df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ¤– RAG AGENT - Retrieval Augmented Generation\n",
      "==================================================\n",
      "ðŸ“š Knowledge Base: NBA 2024-25 Season Summary\n",
      "ðŸ’¬ Type 'exit' or 'quit' to end the conversation\n",
      "==================================================\n",
      "Executing 1 tool call(s)...\n",
      "Calling Tool: retrieve_documents with args: {'query': 'Shai Gilgeous-Alexander points average and Dyson Daniels steals average'}\n",
      "Tool execution successful. Result length: 8307 characters\n",
      "All tools executed. Returning to reasoning agent...\n",
      "Executing 1 tool call(s)...\n",
      "Calling Tool: math_calculator with args: {'operation': 'add', 'a': 32.7, 'b': 3}\n",
      "Tool execution successful. Result length: 17 characters\n",
      "All tools executed. Returning to reasoning agent...\n",
      "\n",
      "==================================================\n",
      "ðŸ¤– AGENT RESPONSE:\n",
      "==================================================\n",
      "Shai Gilgeous-Alexander averaged 32.7 points per game, and Dyson Daniels averaged 3.0 steals per game. Adding these two averages together results in 35.7.\n"
     ]
    }
   ],
   "source": [
    "run_rag_agent_with_memory(memory_length=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
